{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2: Análisis Inicial y Selección de Problema\n",
    "\n",
    "### **Objetivo:** Realizar un análisis exploratorio de datos (EDA) inicial para al menos cuatro conjuntos de datos, diagnosticar y elegir una problemática específica para abordar (regresión, clasificación, clusterización, predicción). Entregar un repositorio con el dataset elegido, el EDA inicial y la problemática seleccionada.\n",
    "\n",
    "### Parte I: Búsqueda y Análisis de Conjuntos de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Instrucciones\n",
    "\n",
    "### Búsqueda de Conjuntos de Datos:\n",
    "\n",
    "- Buscar al menos cuatro conjuntos de datos en plataformas como Kaggle, UCI Machine Learning Repository o cualquier otra fuente confiable.\n",
    "- Asegurarse de que los conjuntos de datos seleccionados sean diversos y cubran diferentes dominios y tipos de datos.\n",
    "\n",
    "### 1. [**S&P 500 Stocks**](https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks)\n",
    "  - Create a time series regression model to predict S&P value and/or stock prices.\n",
    "Explore the most the returns, components and volatility of the S&P 500 index.\n",
    "Identify high and low performance stocks among the list. \n",
    "  - The Standard and Poor's 500 or S&P 500 is the most famous financial benchmark in the world.\n",
    "\n",
    "  - This stock market index tracks the performance of 500 large companies listed on stock exchanges in the United States. As of December 31, 2020, more than $5.4 trillion was invested in assets tied to the performance of this index.\n",
    "\n",
    "  - Because the index includes multiple classes of stock of some constituent companies—for example, Alphabet's Class A (GOOGL) and Class C (GOOG)—there are actually 505 stocks in the gauge.\n",
    "\n",
    "### 2. [**Gold Price Prediction**](https://www.kaggle.com/datasets/franciscogcc/financial-data)\n",
    "  - The data is a time series dataset with financial info for some market indices, commodities, economic indicators and forex rates. Market indices and commodities are represented via the respective exchange traded fund. It includes values from 2010 to 2024.\n",
    "\n",
    "In real world applications, sometimes data will come in different granularities. In this dataset we can find daily, monthly and trimonthly data. Normalizing this inconsistencies and handling nan values should be one of the first challenges when dealing with this dataset.\n",
    "\n",
    "#### About this file\n",
    "\n",
    "Add Suggestion\n",
    "Time series dataset with financial info for the following:\n",
    "\n",
    "- S&P 500 - SPDR S&P 500 ETF Trust\n",
    "\n",
    "- Nasdaq 100 - Invesco QQQ ETF\n",
    "\n",
    "- US Interest Rates - monthly federal rates\n",
    "\n",
    "- CPI - Consumer Price Index, monthly\n",
    "\n",
    "- USD / CHF forex rate\n",
    "\n",
    "- EUR / USD forex rate\n",
    "\n",
    "- GDP - Gross Domestic Product, trimonthly\n",
    "\n",
    "- Silver - abrdn Physical Silver Shares ETF\n",
    "\n",
    "- Oil - USO ETF\n",
    "\n",
    "- Platinum - abrdn Physical Platinum Shares ETF\n",
    "\n",
    "- Palladium - abrdn Physical Palladium Shares ETF\n",
    "\n",
    "- Gold - SPDR Gold Trust ETF\n",
    "\n",
    "- Data obtained via Alpha Vantage and FRED API services\n",
    "\n",
    "### 3. [**Global Daily Port Activity and Trade Estimates**](https://www.kaggle.com/datasets/arunvithyasegar/daily-port-activity-data-and-trade-estimates)\n",
    "\n",
    "  - Daily transit calls and estimates of transit trade volumes (in metric tons) for 24 chokepoints around the world.\n",
    "\n",
    "- last update: 11/11/2024\n",
    "\n",
    "#### Key Features of the Dataset\n",
    "- Port Activity Data: Includes daily logs of port activities, detailing the number of vessels, types of vessels (such as container ships, tankers, bulk carriers), and their activities at each port.\n",
    "\n",
    "- Trade Estimates: Provides estimates on trade volumes for different cargo types, including containerized cargo, bulk goods, and other materials.\n",
    "\n",
    "- Geographic Coverage: Covers multiple ports around the world, making it ideal for global trade analysis.\n",
    "\n",
    "- Temporal Coverage: Offers daily data, allowing for time series analysis to detect trends, seasonal patterns, and anomalies in port activities.\n",
    "\n",
    "- Potential Use Cases\n",
    "Supply Chain and Logistics Optimization: The dataset helps in understanding port congestion, turnaround times, and vessel traffic, assisting in improving logistics and supply chain management.\n",
    "\n",
    "- Economic Analysis: By analyzing the flow of goods, it can provide insights into economic activities, especially in import/export-heavy regions.\n",
    "\n",
    "- Trade Route Planning: Useful for companies planning trade routes, as it highlights busy ports and peak times for port activities.\n",
    "\n",
    "- Predictive Modeling: The data can be used to build models to predict trade volumes, port congestion, and other aspects crucial for stakeholders in the shipping industry.\n",
    "\n",
    "- Possible Analyses\n",
    "\n",
    "- Port Efficiency Comparisons: Compare how different ports manage their daily vessel traffic and cargo handling.\n",
    "\n",
    "- Trade Volume Forecasting: Use historical data to predict future trade volumes based on trends.\n",
    "\n",
    "- Seasonality and Trends: Identify seasonal trends and fluctuations in global trade and port activities.\n",
    "This dataset is valuable for researchers, data analysts, economists, and professionals in the maritime and logistics industries who are interested in understanding and optimizing global trade flows and port operations.\n",
    "\n",
    "### 4. [**Usual Intakes from Food for Energy**](https://www.kaggle.com/datasets/mahdiehhajian/usual-intakes-from-food-for-energy)\n",
    "  - Some of these energy-generating nutrients are listed below:\n",
    "\n",
    "- Vitamin B (B);\n",
    "- carnitine;\n",
    "- Coenzyme Q10 (CoQ10);\n",
    "- keratin;\n",
    "- iron;\n",
    "- magnesium;\n",
    "- protein;\n",
    "- potassium.\n",
    "\n",
    "When looking at this group of fatigue fighters, you should also consider carbohydrates and protein. Carbohydrates from sugary foods and grains give you instant energy;\n",
    "\n",
    "The Nutrition Surveillance Data Tool presents usual intakes of energy, nutrients and other dietary components using data collected in the 2015 Canadian Community Health Survey – Nutrition (CCHS – Nutrition). The goal is to complement the full usual intake dataset, also provided below, by presenting key data in an interactive and user-friendly way.\n",
    "\n",
    "[Health Canada (2019).Usual Intakes from Food for Energy, Nutrients and Other Dietary Components (2004 and 2015 CCHS-Nutrition) derived from Statistics Canada's 2004 and 2015 Canadian Community Health Survey, Nutrition, Share file. Ottawa.](https://www.kaggle.com/datasets/mahdiehhajian/usual-intakes-from-food-for-energy)\n",
    "\n",
    "- Asegurarse de que los conjuntos de datos seleccionados sean diversos y cubran diferentes dominios y tipos de datos.\n",
    "\n",
    "#### Análisis Exploratorio de Datos (EDA) Inicial:\n",
    "\n",
    "- Realizar un EDA inicial para cada uno de los cuatro conjuntos de datos seleccionados.\n",
    "- Incluir visualizaciones, análisis estadístico descriptivo, identificación de valores nulos y outliers.\n",
    "- Documentar los hallazgos de cada EDA en un notebook de Jupyter.\n",
    "\n",
    "#### Diagnóstico y Selección de Problema:\n",
    "\n",
    "- Basándose en el EDA inicial, diagnosticar las principales características y desafíos de cada conjunto de datos.\n",
    "- Elegir una problemática específica para abordar (regresión, clasificación, clusterización, predicción).\n",
    "- Justificar la elección del problema y explicar por qué es relevante y desafiante.\n",
    "\n",
    "#### Creación del Repositorio en GitHub:\n",
    "- Crear un repositorio en GitHub para el Proyecto 2.\n",
    "- Incluir el EDA inicial de los cuatro conjuntos de datos en notebooks separados.\n",
    "- Incluir una carpeta para el dataset elegido con su EDA correspondiente.\n",
    "- Documentar la problemática seleccionada y justificar la elección en un archivo README.md.\n",
    "- Detalles del EDA Inicial\n",
    "\n",
    "#### Descripción del Conjunto de Datos:\n",
    "\n",
    "- Breve descripción de cada conjunto de datos, incluyendo la fuente, el tamaño y las variables.\n",
    "\n",
    "#### Análisis Estadístico Descriptivo:\n",
    "\n",
    "Calcular estadísticas descriptivas básicas (media, mediana, desviación estándar, etc.) para las variables numéricas.\n",
    "Analizar la distribución de las variables categóricas.\n",
    "Visualizaciones:\n",
    "Crear visualizaciones para entender la distribución de las variables (histogramas, gráficos de barras, box plots, etc.).\n",
    "Visualizar las correlaciones entre variables (mapa de calor de correlación).\n",
    "Identificación de Valores Nulos y Outliers:\n",
    "Detectar valores nulos y discutir cómo podrían ser tratados.\n",
    "Identificar outliers y evaluar su impacto potencial en el análisis.\n",
    "Resumen de Hallazgos:\n",
    "Resumir los principales hallazgos de cada EDA, destacando las características y desafíos únicos de cada conjunto de datos.\n",
    "Ejemplo de Estructura del Repositorio en GitHub\n",
    "\n",
    "/Proyecto2\n",
    "|-- /datasets\n",
    "|   |-- dataset1.csv\n",
    "|   |-- dataset2.csv\n",
    "|   |-- dataset3.csv\n",
    "|   |-- dataset4.csv\n",
    "|-- /EDA\n",
    "|   |-- EDA_dataset1.ipynb\n",
    "|   |-- EDA_dataset2.ipynb\n",
    "|   |-- EDA_dataset3.ipynb\n",
    "|   |-- EDA_dataset4.ipynb\n",
    "|-- /selected_dataset\n",
    "|   |-- selected_dataset.csv\n",
    "|   |-- EDA_selected_dataset.ipynb\n",
    "|-- README.md\n",
    "\n",
    "Contenido del Archivo README.md\n",
    "\n",
    "Título del Proyecto: Análisis Inicial y Selección de Problema\n",
    "Descripción: Breve descripción del objetivo del proyecto y su importancia.\n",
    "Conjuntos de Datos Analizados: Descripción breve de los cuatro conjuntos de datos analizados.\n",
    "Resumen del EDA Inicial: Resumen de los hallazgos principales de los EDA realizados.\n",
    "Problema Seleccionado: Descripción detallada del problema seleccionado, la justificación de la elección y los objetivos específicos.\n",
    "Instrucciones para Ejecutar: Pasos para ejecutar los notebooks y reproducir los resultados.\n",
    "Autores: Nombres y roles de los participantes en el proyecto.\n",
    "Licencia: Información sobre la licencia del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Proyecto 2: Análisis y Selección de Problema \n",
    "\n",
    "### **Objetivo:** Realizar el preprocesamiento de datos y la optimización de modelos de machine learning para el conjunto de datos seleccionado. La meta es elegir la técnica de machine learning más adecuada y optimizar sus hiperparámetros para obtener el mejor rendimiento posible.\n",
    "\n",
    "### Parte II: Preprocesamiento y Optimización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instrucciones Detalladas\n",
    "\n",
    "#### **Parte 1:** Preprocesamiento de Datos\n",
    "\n",
    "#### Limpieza de Datos:\n",
    "\n",
    "- Tratar los valores nulos utilizando técnicas adecuadas (imputación, eliminación, etc.).\n",
    "- Manejar los outliers mediante técnicas de filtrado o transformación.\n",
    "\n",
    "#### Transformación de Columnas:\n",
    "\n",
    "- Utilizar ColumnTransformer para aplicar transformaciones específicas a diferentes columnas.\n",
    "- Realizar codificación de variables categóricas utilizando técnicas como One-Hot Encoding.\n",
    "- Escalar las variables numéricas usando StandardScaler u otros métodos de normalización.\n",
    "\n",
    "#### Creación de Pipelines:\n",
    "\n",
    "- Crear pipelines utilizando Pipeline de sklearn para automatizar el preprocesamiento de datos y asegurar la reproducibilidad.\n",
    "- Incluir todos los pasos de preprocesamiento en el pipeline.\n",
    "\n",
    "\n",
    "#### **Parte 2:** Selección de Técnica de Machine Learning\n",
    "\n",
    "#### Entrenamiento Inicial:\n",
    "\n",
    "- Entrenar múltiples modelos de machine learning (por ejemplo, Regresión Lineal, KNN, Árbol de Decisión, Random Forest, XGBoost, LGBM).\n",
    "- Evaluar los modelos utilizando validación cruzada y seleccionar el modelo con el mejor rendimiento inicial.\n",
    "\n",
    "#### Comparación de Modelos:\n",
    "\n",
    "- Comparar los modelos utilizando métricas de rendimiento relevantes (exactitud, precisión, recall, F1-Score, ROC-AUC, etc.).\n",
    "- Seleccionar la técnica de machine learning más adecuada basándose en las métricas y la naturaleza del problema.\n",
    "\n",
    "\n",
    "#### **Parte 3:** Optimización de Hiperparámetros\n",
    "\n",
    "#### GridSearchCV:\n",
    "\n",
    "- Implementar GridSearchCV para realizar una búsqueda exhaustiva de los mejores hiperparámetros para el modelo seleccionado.\n",
    "- Definir el espacio de búsqueda para los hiperparámetros relevantes.\n",
    "\n",
    "#### RandomizedSearchCV:\n",
    "\n",
    "- Implementar RandomizedSearchCV para realizar una búsqueda aleatoria de los mejores hiperparámetros, especialmente útil si el espacio de búsqueda es grande.\n",
    "\n",
    "#### Optuna:\n",
    "\n",
    "- Implementar Optuna para una optimización avanzada de los hiperparámetros, aprovechando técnicas como la optimización bayesiana y el pruning.\n",
    "\n",
    "#### Evaluación de Modelos Optimizados:\n",
    "\n",
    "- Entrenar el modelo con los mejores hiperparámetros encontrados y evaluar su rendimiento en el conjunto de prueba.\n",
    "- Comparar el rendimiento del modelo optimizado con el modelo inicial.\n",
    "\n",
    "\n",
    "#### **Parte 4:** Documentación y Entrega\n",
    "\n",
    "#### Documentación del Proceso:\n",
    "\n",
    "- Documentar todos los pasos del preprocesamiento, selección de técnica y optimización en un notebook de Jupyter.\n",
    "- Incluir explicaciones detalladas y justificaciones para cada decisión tomada.\n",
    "\n",
    "#### Subida a GitHub:\n",
    "\n",
    "- Actualizar el repositorio de GitHub con los notebooks de preprocesamiento, selección de técnica y optimización.\n",
    "- Incluir los resultados de la optimización y la comparación de modelos.\n",
    "- Crear un tag de liberación (v2.0.0) para esta versión del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_mitic_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
